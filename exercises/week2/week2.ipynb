{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises week 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies needed to run this Notebook (uncomment the lines below to install dependencies if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -m requirements.txt\n",
    "\n",
    "# or\n",
    "#%pip install requests==2.31.0\n",
    "#%pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import **requests** in order to download the tsv files, and numpy and Pandas to work with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Start by downloading these four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    req = requests.get(f'https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data{i}.tsv')\n",
    "    \n",
    "    with open(f'../data/data{i}.tsv', 'w') as f:\n",
    "        f.write(req.text)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load each files content into a DataFrame naming the collums x and y, and then these dataframes into a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {f'df{i}': pd.read_csv(f'https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data{i}.tsv', sep='\\t', names=['x', 'y']) for i in range(1,5)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Using the numpy function mean, calculate the mean of both x-values and y-values for each dataset\n",
    "_(Use python string formatting to print precisely two decimal places of these results to the output cell.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(data.values(), start=1):\n",
    "    x_mean = np.mean(df['x'])\n",
    "    y_mean = np.mean(df['y'])\n",
    "    \n",
    "    print(f'data{i}.tsv file')\n",
    "    print('=================')\n",
    "    print(f'Mean of X values: {x_mean:.2f}')\n",
    "    print(f'Mean of Y values: {y_mean:.2f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Now calculate the variance for all of the various sets of x- and y-values, by using the numpy function var.\n",
    "Print it to three decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(data.values(), start=1):\n",
    "    x_var = np.var(df['x'])     # here is calculated population variance, \n",
    "    y_var = np.var(df['y'])     # if sample variance is preferred a \"ddof=1\" parameter is needed, or use pd.var() instead.\n",
    "    \n",
    "    \n",
    "    print(f'Variance of data in data{i}.tsv file')\n",
    "    print('=================')\n",
    "    print(f'Variation of X values: {x_var:.3f}')\n",
    "    print(f'Variation of Y values: {y_var:.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Use numpy to calculate the Pearson correlation between x- and y-values for all four data sets \n",
    "(also print to three decimal places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(data.values(), start=1):\n",
    "    matrix = np.corrcoef(df['x'], df['y'])\n",
    "    \n",
    "    print(f'PCC of data in data{i}.tsv file')\n",
    "    print('=================')\n",
    "    print(f'PCC between x and y: {matrix[0,1]:.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 use linear regression to fit a straight line f(x)=ax+b through each dataset and report a and b\n",
    "(to two decimal places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(data.values(), start=1):\n",
    "    a, b, r_value, p_value, std_err = stats.linregress(df['x'], df['y'])\n",
    "    \n",
    "    print(f'Linier regression of data in data{i}.tsv file')\n",
    "    print('==============================================')\n",
    "    print(f'The value of a: {a:.2f}')\n",
    "    print(f'The value of b: {b:.2f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Comment on the results from the previous steps. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the datasets are very similar concerning the **Mean** of their x and y data (1.2).  \n",
    "The **Variance** of its x-values is precisely 10 in all datasets, and practically equal for the y-values, with a small difference of .004 between datasets (1, 2), and (3, 4) (1.3).  \n",
    "The **correlation** between x- and y-values in all four datasets is also very close to being equal (1.4).     \n",
    "Regarding the **linear regression** analysis, the slope corresponding to all datasets is 0.5, and the y-intercept is at 3.0 (1.5).  \n",
    "So, even though the actual data in the four datasets differ, they seem equal when viewed through the above tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 plot the four datasets using matplotlib.pyplot\n",
    "* Use a two-by-two subplot to put all of the plots nicely in a grid and \n",
    "* use the same x and y range for all four plots. \n",
    "* And include the linear fit in all four plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, r_value, p_value, std_err = stats.linregress(df['x'], df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 20))  # create a figure (canvas like)\n",
    "plt.subplots_adjust(hspace=0.5)  # addjust the space between the rows\n",
    "\n",
    "for i, df in enumerate(data.values(), start=1):\n",
    "    \n",
    "    a, b, *args = stats.linregress(df['x'], df['y']) # calculate the linear regression\n",
    "    reg = a * df['x'] + b # calculate the y koordiantes for the regression line (slope * x-values * intersction).\n",
    "\n",
    "    # add the data to a subplot\n",
    "    ax = fig.add_subplot(len(df)//2, 2, i) # specify how many rows in the axes in the figure, how many collums (2), and which number of the total the specific plot is \n",
    "    ax.set_title(f'Data {i}')\n",
    "    ax.set_xlim([0, 20]) # set x axis to show from 0 to 20\n",
    "    ax.set_ylim([0, 14]) # set y axis to show from 0 to 14\n",
    "    ax.set_xticks(range(0, 21, 5)) # adjust the x-ticks layout\n",
    "    ax.set_yticks(range(2, 15, 2)) # adjust the y-ticks layout\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    \n",
    "    ax.scatter(df['x'], df['y']) # add data to the chart (x,y)\n",
    "    ax.plot(df['x'], reg, color='red') # add the regression line\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Explain - in your own words - what you think my point with this exercise is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe the purpose of these exercises is partly to gain hands-on experience with Python's numpy, pandas, scipy, and matplotlib libraries, in conjunction with basic statistical operations. They also aim to deepen our understanding of how datasets with identical statistical properties, such as mean, variance, and correlation, can appear similar when subjected to statistical analyses. However, this does not necessarily imply that the datasets are identical or can be interpreted as such. For instance, datasets with the same mean, variance, and correlation may still differ significantly. The visual look of the data on the other hand is clearly showing a difference in what the data tell, and it shows the importance of visual plots when working with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 What did you get out of asking the LLM about the previous sub-question? \n",
    "* How did you even go about asking the LLM about the point of the entire set of questions? \n",
    "* Reflect on whether or not the LLM helped you get smarter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, I used ChatGPT for two things. Instead of using the documentation of Python libraries to figure out which methods were relevant for a specific operation, I used the LLM as a reference. This wasn't much different from what I did earlier with, for instance, Stack Overflow (Google). I have forced myself to understand what I got from the LLM, and not just accept it. For me, it is clearly the best way to learn. First, explain to myself what I want, then ask the LLM how it's done, and then keep on asking \"why\" until I understand the topic.\n",
    "\n",
    "Another thing I used ChatGPT for in this assignment has been to refresh my knowledge about Mean, Variance, PCC, and other basic statistical tools. For this, I also used other online resources, mainly YouTube, but ChatGPT was a help.\n",
    "\n",
    "Regarding the last question (Reflect on whether or not the LLM helped you get smarter?): Then yes, it is a brilliant tutor, especially if one keeps on asking \"why\" to everything until one gets it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Questions for the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What is the difference between data and metadata? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is the raw data collected or given. Meta data is data about data.    \n",
    "Related to \"Anscombe's quartet\" the numbers in the tsv files are the data.    \n",
    "An example of metadata would be the information about how these datasets where created (e.g what is writting in the wikipedia page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 How does that relate to the GPS tracks-example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data would be what is in the xml file. The metadata would be the generated trees, houses an streets in the google earth video example.    \n",
    "The data in it self can be a bit abstract to think about but when put into a context it suddently makes more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sune says that the human eye is a great tool for data analysis. \n",
    "##### Do you agree? Explain why/why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think of the eye as a tool for generating abstractions, it takes in raw caotic data and organises it into a meaningfull, simplified and context related information. Then yes i think it is a great tool for dataanalisys. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mention something that the human eye is very good at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an efficient interface to a more complex dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Can you think of something that is difficult for the human eye. Explain why your example is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When what we look at becomes to detailed it is gennerally more difficult for the eye to grasp. Again the idea of the eye creating an abstraction or an interface to the complex underliying datastructures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simpson's paradox is hard to explain. Come up with your own example - or find one on line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Simpson’s Paradox is a statistical phenomenon where an association between two variables in a population emerges, disappears or reverses when the population is divided into subpopulations. For instance, two variables may be positively associated in a population, but be independent or even negatively associated in all subpopulations.\" - https://plato.stanford.edu/entries/paradox-simpson/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain the differnece between exploratory and explanatory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis**     \n",
    "The porpous of exploratory Data Analysis is to get a broad overview of the domain under investigation.      \n",
    "\n",
    "**Explanatory data analysis**     \n",
    "Is about explaning the data or deliver a message from what you think the data is telling you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualizing patterns in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy pandas Pyarrow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'Helvetica'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/Police_Department_Incident_Reports__Historical_2003_to_May_2018_20240129.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focuscrimes = {'WEAPON LAWS', 'PROSTITUTION', 'DRIVING UNDER THE INFLUENCE', 'ROBBERY', 'BURGLARY', 'ASSAULT', 'DRUNKENNESS', 'DRUG/NARCOTIC', 'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'VEHICLE THEFT', 'STOLEN PROPERTY', 'DISORDERLY CONDUCT'}\n",
    "focuscrimes = pd.Series(sorted(focuscrimes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['Date']) # convert from text to datetime\n",
    "data['year'] = data['date'].dt.year         # get the year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Weekly patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_order = {\n",
    "    'Monday': 1,\n",
    "    'Tuesday': 2,\n",
    "    'Wednesday': 3,\n",
    "    'Thursday': 4,\n",
    "    'Friday': 5,\n",
    "    'Saturday': 6,\n",
    "    'Sunday': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 30))  # create a figure (canvas like)\n",
    "fig.suptitle('Number of crimes per week-day by category', fontsize=28) # add the main title to the figure on top of all plots\n",
    "\n",
    "for i, category in enumerate(focuscrimes, start=1):\n",
    "\n",
    "    crime_data = data[data['Category'] == category] # filter the dataset by category    \n",
    "    crime_counts_per_day = crime_data['DayOfWeek'].value_counts()#.sort_index() # count the crimes per week-day for this category\n",
    "    \n",
    "    crime_counts_per_day = crime_counts_per_day.reindex(sorted(crime_counts_per_day.index, key=lambda day: weekday_order[day])) # sort the in a normal week order\n",
    "\n",
    "    # add the data to a subplot\n",
    "    ax = fig.add_subplot(len(focuscrimes)//2, 2, i) # specify how many rows in the axes in the figure, how many collums (2), and which number of the total the specific plot is \n",
    "    \n",
    "    ax.margins(y=0.2) # margin from bars to top of the axes \n",
    "    ax.text(0.1, 0.95, category, ha='left', va='top', transform=ax.transAxes, fontsize=12) # add title text inside the axes\n",
    "    ax.bar(crime_counts_per_day.index, crime_counts_per_day.values, width=0.5) # add data to the chart (x,y)\n",
    "    \n",
    "    # add / remove years and years-title on all or the last 2\n",
    "    ax.set_xticklabels([]) if i < len(focuscrimes)-1 else ax.set_xlabel('Day of week')\n",
    "\n",
    "    if i % 2 != 0:\n",
    "        ax.set_ylabel('Occorences')  # y-label on all plots in the left row only\n",
    "\n",
    "plt.tight_layout(pad=3.0)  # Adjusts the subplots to fit into the figure area. pad adds space between fig title and the sup charts\n",
    "plt.show()  # Display the figure with the subplots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 The months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check if some months are worse by counting up number of crimes in Jan, Feb, ..., Dec. Did you see any surprises there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['Date'])\n",
    "data['month'] = data['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 30))  # create a figure (canvas like)\n",
    "fig.suptitle('Number of crimes per month by category', fontsize=28) # add the main title to the figure on top of all plots\n",
    "\n",
    "for i, category in enumerate(focuscrimes, start=1):\n",
    "\n",
    "    crime_data = data[data['Category'] == category] # filter the dataset by category    \n",
    "    crime_counts_per_month = crime_data['month'].value_counts().sort_index() # count the crimes per year for this category\n",
    "    \n",
    "    #crime_counts_per_day = crime_counts_per_day.reindex(sorted(crime_counts_per_day.index, key=lambda day: weekday_order[day])) # sort the in a normal week order\n",
    "\n",
    "    # add the data to a subplot\n",
    "    ax = fig.add_subplot(len(focuscrimes)//2, 2, i) # specify how many rows in the axes in the figure, how many collums (2), and which number of the total the specific plot is \n",
    "    \n",
    "    ax.margins(y=0.2) # margin from bars to top of the axes \n",
    "    ax.text(0.1, 0.95, category, ha='left', va='top', transform=ax.transAxes, fontsize=12) # add title text inside the axes\n",
    "    ax.bar(crime_counts_per_month.index, crime_counts_per_month.values, width=0.5) # add data to the chart (x,y)\n",
    "    \n",
    "    # add / remove years and years-title on all or the last 2\n",
    "    ax.set_xticklabels([]) if i < len(focuscrimes)-1 else ax.set_xlabel('Month')\n",
    "\n",
    "    if i % 2 != 0:\n",
    "        ax.set_ylabel('Occorences')  # y-label on all plots in the left row only\n",
    "\n",
    "plt.tight_layout(pad=3.0)  # Adjusts the subplots to fit into the figure area. pad adds space between fig title and the sup charts\n",
    "plt.show()  # Display the figure with the subplots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments on the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The 24 hour cycle\n",
    "We can also forget about weekday and simply count up the number of each crime-type that occurs in the dataset from midnight to 1am, 1am - 2am ... and so on.     \n",
    "Again: Give me a couple of comments on what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['time'] = pd.to_datetime(data['Time'])\n",
    "# data['time'] = pd.to_datetime(data['Time'], format='%H:%M')\n",
    "# data['time'] = data['time'].dt.time\n",
    "# data['time_numerical'] = data['time'].apply(lambda t: t.hour * 60 + t.minute)\n",
    "data['hour'] = pd.to_datetime(data['Time'], format='%H:%M').dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 30))  # create a figure (canvas like)\n",
    "fig.suptitle('Number of crimes per month by category', fontsize=28) # add the main title to the figure on top of all plots\n",
    "\n",
    "for i, category in enumerate(focuscrimes, start=1):\n",
    "\n",
    "    crime_data = data[data['Category'] == category] # filter the dataset by category    \n",
    "    #crime_counts_per_time = crime_data['time_numerical'].value_counts().sort_index() # count the crimes per year for this category\n",
    "    \n",
    "    crime_counts_per_hour = crime_data.groupby('hour').size()\n",
    "    crime_counts_per_hour = crime_counts_per_hour.reindex(range(24), fill_value=0)\n",
    "    \n",
    "\n",
    "\n",
    "    # add the data to a subplot\n",
    "    ax = fig.add_subplot(len(focuscrimes)//2, 2, i) # specify how many rows in the axes in the figure, how many collums (2), and which number of the total the specific plot is \n",
    "    \n",
    "    ax.margins(y=0.2) # margin from bars to top of the axes \n",
    "    ax.text(0.1, 0.95, category, ha='left', va='top', transform=ax.transAxes, fontsize=12) # add title text inside the axes\n",
    "    ax.bar(crime_counts_per_hour.index, crime_counts_per_hour.values) # add data to the chart (x,y)\n",
    "    \n",
    "    ax.set_xticks(range(24))  # Set ticks for each hour\n",
    "    ax.set_xticklabels([f'{hour:02d}-{(hour+1)%24:02d}' for hour in range(24)], rotation=90)  # Format labels as 'HH-HH'\n",
    "\n",
    "\n",
    "    # add / remove years and years-title on all or the last 2\n",
    "    #ax.set_xticklabels([]) if i < len(focuscrimes)-1 else ax.set_xlabel('Time')\n",
    "\n",
    "    if i % 2 != 0:\n",
    "        ax.set_ylabel('Occorences')  # y-label on all plots in the left row only\n",
    "\n",
    "plt.tight_layout(pad=3.0)  # Adjusts the subplots to fit into the figure area. pad adds space between fig title and the sup charts\n",
    "plt.show()  # Display the figure with the subplots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All crimesrates seams to be low in the early morning around 5-6    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hours of the week. \n",
    "But by looking at just 24 hours, we may be missing some important trends that can be modulated by week-day, so let's also check out the 168 hours of the week. So let's see the number of each crime-type Monday night from midninght to 1am, Monday night from 1am-2am - all the way to Sunday night from 11pm to midnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_order = {\n",
    "    'Monday': 0,\n",
    "    'Tuesday': 1,\n",
    "    'Wednesday': 2,\n",
    "    'Thursday': 3,\n",
    "    'Friday': 4,\n",
    "    'Saturday': 5,\n",
    "    'Sunday': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['day_num'] = data['DayOfWeek'].map(weekday_order)\n",
    "data['hour_of_week'] = data['day_num'] * 24 + data['hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 30))  # create a figure (canvas like)\n",
    "fig.suptitle('Number of crimes per hour of the week by category', fontsize=28) # add the main title to the figure on top of all plots\n",
    "\n",
    "for i, category in enumerate(focuscrimes, start=1):\n",
    "\n",
    "    crime_data = data[data['Category'] == category] # filter the dataset by category    \n",
    "    \n",
    "    crime_counts_per_hour_of_week = crime_data.groupby('hour_of_week').size()\n",
    "    crime_counts_per_hour_of_week = crime_counts_per_hour_of_week.reindex(range(168), fill_value=0)\n",
    "\n",
    "    # add the data to a subplot\n",
    "    ax = fig.add_subplot(len(focuscrimes)//2, 2, i) # specify how many rows in the axes in the figure, how many collums (2), and which number of the total the specific plot is \n",
    "    \n",
    "    ax.margins(y=0.2) # margin from bars to top of the axes \n",
    "    ax.text(0.1, 0.95, category, ha='left', va='top', transform=ax.transAxes, fontsize=12) # add title text inside the axes\n",
    "    ax.bar(crime_counts_per_hour_of_week.index, crime_counts_per_hour_of_week.values) # add data to the chart (x,y)\n",
    "    \n",
    "    ax.set_xticks(range(0, 168, 24))  # Set ticks for the start of each day\n",
    "    ax.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'], rotation=45)  # Days of the week\n",
    "\n",
    "    # add / remove years and years-title on all or the last 2\n",
    "    ax.set_xticklabels([]) if i < len(focuscrimes)-1 else ax.set_xlabel('Day of week')\n",
    "\n",
    "    if i % 2 != 0:\n",
    "        ax.set_ylabel('Occorences')  # y-label on all plots in the left row only\n",
    "\n",
    "plt.tight_layout(pad=3.0)  # Adjusts the subplots to fit into the figure area. pad adds space between fig title and the sup charts\n",
    "plt.show()  # Display the figure with the subplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating nice plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a list of 10 rules for nice plots based on the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write explanatory **captions** to your plots\n",
    "1. Be aware on the zoom level of your plots. It should make sence.\n",
    "1. Have values that are compareble between related plots, or explain to the reader what he should be aware about if its not compareble.\n",
    "1. Reduce the noise as much as possible. (colors, background etc.)\n",
    "1. Name the axes\n",
    "1. have meaningfull ranges of values in labels\n",
    "1. Have a nicer font than default\n",
    "1. Make an effort to make it look nice\n",
    "1. Tighten your subplots (remove unnessesary spaces in between)\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Exploring other types of plots for temporal data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calendar plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install calplot pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calplot\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "data = pd.read_csv(\"../data/Police_Department_Incident_Reports__Historical_2003_to_May_2018_20240129.csv\")\n",
    "\n",
    "# Convert the 'Date' column to datetime objects\n",
    "# Make sure to use the correct column name as it appears in your CSV file\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Create a Series with the count of incidents per day\n",
    "incident_counts = data['Date'].value_counts().sort_index()\n",
    "\n",
    "# Plot the calendar heatmap\n",
    "calplot.calplot(incident_counts, cmap='YlGn', suptitle='Incidents Calendar Plot')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polar bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "# Compute pie slices\n",
    "N = 20\n",
    "theta = np.linspace(0.0, 2 * np.pi, N, endpoint=False)\n",
    "radii = 10 * np.random.rand(N)\n",
    "width = np.pi / 4 * np.random.rand(N)\n",
    "colors = plt.cm.viridis(radii / 10.)\n",
    "\n",
    "ax = plt.subplot(projection='polar')\n",
    "ax.bar(theta, radii, width=width, bottom=0.0, color=colors, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say the day of the week is stored in a column named 'DayOfWeek'\n",
    "# We will create a mapping of days to numerical values (Monday=0, Sunday=6)\n",
    "day_mapping = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6}\n",
    "data['DayOfWeek'] = data['DayOfWeek'].map(day_mapping)\n",
    "\n",
    "# Count the number of incidents for each day of the week\n",
    "day_counts = data['DayOfWeek'].value_counts().sort_index()\n",
    "\n",
    "# Create a polar plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Compute the angle for each bar\n",
    "theta = np.linspace(0.0, 2 * np.pi, len(day_counts), endpoint=False)\n",
    "\n",
    "# Create the bars\n",
    "bars = ax.bar(theta, day_counts, align='center', alpha=0.5)\n",
    "\n",
    "# Set the labels for each bar (day of the week)\n",
    "ax.set_xticks(theta)\n",
    "ax.set_xticklabels(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "\n",
    "# Set the title and show the plot\n",
    "ax.set_title('Number of Incidents by Day of the Week')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'DayOfWeek' and 'Category' columns exist\n",
    "# Group by 'DayOfWeek' and 'Category' and count the incidents\n",
    "grouped_data = data.groupby(['DayOfWeek', 'Category']).size().reset_index(name='counts')\n",
    "\n",
    "# Pivot the data to have days of the week as the index and categories as columns\n",
    "pivoted_data = grouped_data.pivot(index='DayOfWeek', columns='Category', values='counts').fillna(0)\n",
    "\n",
    "# Create a mapping of days to numerical values (Monday=0, Sunday=6)\n",
    "day_mapping = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6}\n",
    "pivoted_data = pivoted_data.reindex(day_mapping.keys())  # Ensure the days are in the correct order\n",
    "\n",
    "# Create a polar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Compute the angle for each bar\n",
    "theta = np.linspace(0.0, 2 * np.pi, len(pivoted_data), endpoint=False)\n",
    "\n",
    "# Width of each bar\n",
    "width = (2 * np.pi) / len(pivoted_data)\n",
    "\n",
    "# Plot a bar for each category, for each day\n",
    "bottom = np.zeros(len(pivoted_data))  # Initialize the bottom of the stack\n",
    "for category in pivoted_data.columns:\n",
    "    heights = pivoted_data[category]\n",
    "    bars = ax.bar(theta, heights, width=width, bottom=bottom, label=category, alpha=0.5)\n",
    "    bottom += heights  # Update the bottom for the next stack\n",
    "\n",
    "# Set the labels for each bar (day of the week)\n",
    "ax.set_xticks(theta)\n",
    "ax.set_xticklabels(day_mapping.keys())\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.5, 1.1))\n",
    "\n",
    "# Set the title and show the plot\n",
    "ax.set_title('Number of Incidents by Category and Day of the Week')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Set the 'Date' column as the index of the DataFrame\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Select a category to analyze, e.g., 'ASSAULT'\n",
    "category_data = data[data['Category'] == 'ASSAULT']\n",
    "\n",
    "# Resample the data to get the monthly count of incidents for the selected category\n",
    "monthly_counts = category_data.resample('M').size()\n",
    "\n",
    "# Plot the trend of the selected category over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "monthly_counts.plot()\n",
    "plt.title('Monthly Trend of Assault Incidents')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Incidents')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What did you learn from using LLM's to simply solve everything in this exercise (in contrast to the previous ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have less detailed knowledge about the code. in fact i did not focus on it at all. But my focus was on the result and on how i in Natual language could adjust the code/charts to fit to what i want. My previous knowledge came in handy when trying to adjust the code given by the LLM, but i could have focused on Natal Language only and getting the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Back to visualizing patterns in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The types of crime and how they take place across San Francisco's police districts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we'll be combining information about PdDistrict and Category to explore differences between SF's neighborhoods.     \n",
    "**First, simply list the names of SF's 10 police districts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/Police_Department_Incident_Reports__Historical_2003_to_May_2018_20240129.csv\")\n",
    "districts = set(data['PdDistrict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which has the most crimes?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby('PdDistrict')\n",
    "crime_counts = grouped.size()\n",
    "crime_counts.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most crimes occour in SOUTHERN, which have a total of 390692 crimes over the whole period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which has the most focus crimes?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focuscrimes = {'WEAPON LAWS', 'PROSTITUTION', 'DRIVING UNDER THE INFLUENCE', 'ROBBERY', 'BURGLARY', 'ASSAULT', 'DRUNKENNESS', 'DRUG/NARCOTIC', 'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'VEHICLE THEFT', 'STOLEN PROPERTY', 'DISORDERLY CONDUCT'}\n",
    "focuscrimes = pd.Series(sorted(focuscrimes))\n",
    "filtered_data = data[data['Category'].isin(focuscrimes)]\n",
    "grouped = filtered_data.groupby('PdDistrict')\n",
    "crime_counts = grouped.size().sort_values(ascending=False)\n",
    "crime_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most crimes within the focuscrimes set also occour in SOUTHERN, which have a total of 226805 crimes over the whole period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I'm interested to know if there are certain crimes that happen much more in certain neighborhoods than what's typical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to calculate the relative probabilities of seeing each type of crime in the dataset as a whole.     \n",
    "That's simply a normalized version of this plot. Let's call it P(crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby('Category')\n",
    "p_crime = grouped.size().sort_values(ascending=False) / len(data)\n",
    "p_crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see each crime category and its properbillity. The probability of observing each type of crime in the dataset as a whole. P(crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate that same probability distribution but for each PD district, let's call that P(crime|district)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby(['PdDistrict', 'Category'])\n",
    "crime_counts = grouped.size().reset_index(name='counts')\n",
    "prop_of_crime_district = grouped.size() / crime_counts.groupby('PdDistrict')['counts'].sum()\n",
    "prop_of_crime_district"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT Feedback solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby(['PdDistrict', 'Category'])\n",
    "crime_counts = grouped.size().reset_index(name='counts')\n",
    "total_crimes_per_district = crime_counts.groupby('PdDistrict')['counts'].sum()\n",
    "crime_counts['total_district_crimes'] = crime_counts['PdDistrict'].map(total_crimes_per_district)\n",
    "crime_counts['prop_crime_district'] = crime_counts['counts'] / crime_counts['total_district_crimes']\n",
    "crime_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the ratio **P(crime|district)/P(crime)**. That ratio is equal to 1 if the crime occurs at the same level within a district as in the city as a whole. If it's greater than one, it means that the crime occurs more frequently within that district. If it's smaller than one, it means that the crime is rarer within the district in question than in the city as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_of_crime_district_p_crime = prop_of_crime_district / p_crime\n",
    "prop_of_crime_district_p_crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each district plot these ratios for the 14 focus crimes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(crime)\n",
    "grouped = data.groupby('Category')\n",
    "p_crime = grouped.size().sort_values(ascending=False) / len(data)\n",
    "\n",
    "# P(crime|district)\n",
    "grouped = data.groupby(['PdDistrict', 'Category'])\n",
    "crime_counts = grouped.size().reset_index(name='counts')\n",
    "prop_of_crime_district = grouped.size() / crime_counts.groupby('PdDistrict')['counts'].sum()\n",
    "\n",
    "#  \n",
    "\n",
    "\n",
    "focuscrimes = {'WEAPON LAWS', 'PROSTITUTION', 'DRIVING UNDER THE INFLUENCE', 'ROBBERY', 'BURGLARY', 'ASSAULT', 'DRUNKENNESS', 'DRUG/NARCOTIC', 'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'VEHICLE THEFT', 'STOLEN PROPERTY', 'DISORDERLY CONDUCT'}\n",
    "focuscrimes = pd.Series(sorted(focuscrimes))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 30))  # create a figure (canvas like)\n",
    "fig.suptitle('P(crime|district)/P(crime)', fontsize=28) # add the main title to the figure on top of all plots\n",
    "\n",
    "for i in range(1, 14):\n",
    "    ax = fig.add_subplot(len(focuscrimes)//2, 2, i)\n",
    "    \n",
    "plt.show()  # Display the figure with the subplots.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
